{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\owner\\anaconda3\\envs\\tf_env\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\owner\\anaconda3\\envs\\tf_env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\owner\\anaconda3\\envs\\tf_env\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\owner\\anaconda3\\envs\\tf_env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\owner\\anaconda3\\envs\\tf_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\tf_env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk  # Natural Language Toolkit\n",
    "from nltk.tokenize import word_tokenize  # To split sentences into words\n",
    "from nltk.corpus import stopwords  # To remove common words like \"the\", \"is\"\n",
    "from nltk.probability import FreqDist  # To calculate frequency of words\n",
    "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to numerical data\n",
    "from sklearn.naive_bayes import MultinomialNB  # A simple classifier for sentiment analysis\n",
    "from sklearn.metrics import accuracy_score, classification_report  # To evaluate the model\n",
    "\n",
    "# Step 1: Download required NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer for splitting text into words\n",
    "nltk.download('stopwords')  # Pre-defined list of common stopwords\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "# Step 2: Create a small dataset\n",
    "# Dataset with sentences labeled as positive or negative\n",
    "data = [\n",
    "    (\"I love this product, it is amazing!\", \"positive\"),\n",
    "    (\"This is the worst experience I've ever had.\", \"negative\"),\n",
    "    (\"I feel great using this app!\", \"positive\"),\n",
    "    (\"The service was horrible and disappointing.\", \"negative\"),\n",
    "    (\"What a fantastic experience, I am very happy!\", \"positive\"),\n",
    "    (\"I hate this, it's a complete waste of time.\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Step 3: Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))  # Get English stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)  # Return cleaned text as a single string\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "cleaned_data = [(preprocess_text(sentence), label) for sentence, label in data]\n",
    "\n",
    "# Step 4: Prepare data for training\n",
    "# Split the sentences and their labels\n",
    "sentences, labels = zip(*cleaned_data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 5: Convert text data to numerical format\n",
    "# CountVectorizer converts text into a matrix of word counts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)  # Fit and transform training data\n",
    "X_test_vectorized = vectorizer.transform(X_test)  # Only transform testing data\n",
    "\n",
    "# Step 6: Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()  # Initialize the classifier\n",
    "classifier.fit(X_train_vectorized, y_train)  # Train the model\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "# Predict sentiment for test data\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Print accuracy and detailed report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Test with new sentences\n",
    "new_sentences = [\"I absolutely love this!\", \"This is terrible and awful.\"]\n",
    "new_sentences_cleaned = [preprocess_text(sentence) for sentence in new_sentences]\n",
    "new_sentences_vectorized = vectorizer.transform(new_sentences_cleaned)\n",
    "predictions = classifier.predict(new_sentences_vectorized)\n",
    "\n",
    "# Display results\n",
    "for sentence, sentiment in zip(new_sentences, predictions):\n",
    "    print(f\"'{sentence}' -> Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
